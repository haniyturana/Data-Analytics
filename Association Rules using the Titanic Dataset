-----------------------------------------------------------------------------------------------------------------------
R
------------------------------------------------------------------------------------------------------------------------
#########################################
# ASSOCIATION RULE MINING - TITANIC DATA
#########################################

# ---------------------------------------
# 1. Load Data & Inspect
# ---------------------------------------
# List all objects loaded from .rdata file
ls()

# Look at first few rows of raw data
head(titanic.raw)

# Check structure of data
str(titanic.raw)

# Convert to transactions (required format for association rules)
titanic_trans <- as(titanic.raw, "transactions")
inspect(head(titanic_trans))

# ---------------------------------------
# 2. Descriptive Statistics of Transactions
# ---------------------------------------
# Summary statistics of transaction data
summary(titanic_trans)

# ---------------------------------------
# 3. Generate Association Rules (Support & Confidence thresholds)
# ---------------------------------------
library(arules)
library(arulesViz)

# Create rules with minimum support = 0.1, confidence = 0.6
Aturan.S1 <- apriori(titanic_trans, parameter = list(supp=0.1, conf=0.6))

# View all rules
inspect(Aturan.S1)

# Sort rules by confidence (descending)
Aturan.S1 <- sort(Aturan.S1, by="confidence", decreasing=TRUE)

# View sorted rules
inspect(head(Aturan.S1, 10))

# ---------------------------------------
# 4. Visualize Rules
# ---------------------------------------
# Node graph visualization
plot(Aturan.S1, method="graph")

# Parallel coordinates plot
plot(Aturan.S1, method="paracoord", control=list(reorder=TRUE))

# ---------------------------------------
# 5. Rules for Individuals Who Survived (RHS = "Survived=Yes")
# ---------------------------------------
Aturan.S2 <- apriori(titanic_trans, 
                     parameter=list(supp=0.1, conf=0.6),
                     appearance=list(default="lhs", rhs="Survived=Yes"),
                     control=list(verbose=FALSE))

# Sort rules by confidence
Aturan.S2 <- sort(Aturan.S2, by="confidence", decreasing=TRUE)

# View rules
inspect(Aturan.S2)

# Visualization
plot(Aturan.S2, method="graph")
windows(10,10)
plot(Aturan.S2, method="paracoord", control=list(reorder=TRUE))

# ---------------------------------------
# 6. Rules for Survival by Class & Age
# ---------------------------------------
# LHS: Sex, Class, Age; RHS: Survived=Yes
Aturan.S3 <- apriori(titanic_trans,
                     parameter=list(supp=0.01, conf=0.6),
                     appearance=list(
                       lhs=c("Sex=Male","Sex=Female","Class=1st","Class=2nd","Class=3rd",
                             "Age=Child","Age=Adult"),
                       rhs="Survived=Yes"),
                     control=list(verbose=FALSE))

# Sort rules by confidence
Aturan.S3 <- sort(Aturan.S3, by="confidence", decreasing=TRUE)

# View rules
inspect(Aturan.S3)

# Visualization
plot(Aturan.S3, method="graph")
windows(10,10)
plot(Aturan.S3, method="paracoord", control=list(reorder=TRUE))

# ---------------------------------------
# Notes:
# - "lhs" = Left-hand side (conditions)
# - "rhs" = Right-hand side (consequent, outcome)
# - Support: fraction of transactions containing both lhs and rhs
# - Confidence: likelihood that rhs occurs when lhs occurs
# - Use visualization to detect relationships and patterns
# - Adjust support & confidence thresholds if too many/few rules


------------------------------------------------------------------------------------------------------------------------
PYTHON
------------------------------------------------------------------------------------------------------------------------

#########################################
# ASSOCIATION RULE MINING - TITANIC DATA - in Python
#########################################

# ---------------------------------------
# 1. Load Data & Inspect
# ---------------------------------------
import pandas as pd

titanic_raw = pd.read_csv("titanic.csv")  # adjust file if needed
print("Head of data:\n", titanic_raw.head())
print("Data info:\n", titanic_raw.info())

# One-hot encoding for association rules
titanic_onehot = pd.get_dummies(titanic_raw.astype(str))
print("One-hot encoded sample:\n", titanic_onehot.head())

# ---------------------------------------
# 2. Load Required Libraries
# ---------------------------------------
from mlxtend.frequent_patterns import apriori, association_rules
import matplotlib.pyplot as plt
import networkx as nx
from pandas.plotting import parallel_coordinates

# ---------------------------------------
# 3. Generate All Rules (S1)
# ---------------------------------------
frequent_itemsets_S1 = apriori(titanic_onehot, min_support=0.1, use_colnames=True)
rules_S1 = association_rules(frequent_itemsets_S1, metric="confidence", min_threshold=0.6)
rules_S1 = rules_S1.sort_values(by="confidence", ascending=False)

print("Top 10 rules (S1):\n", rules_S1.head(10))

# Visualization S1 - network graph
G1 = nx.DiGraph()
for _, row in rules_S1.iterrows():
    for ant in row['antecedents']:
        for con in row['consequents']:
            G1.add_edge(ant, con, weight=row['confidence'])

plt.figure(figsize=(10,10))
pos = nx.spring_layout(G1, k=0.5, seed=42)
nx.draw(G1, pos, with_labels=True, node_size=2000, node_color='skyblue', font_size=10, font_weight='bold', arrows=True)
edge_labels = nx.get_edge_attributes(G1, 'weight')
nx.draw_networkx_edge_labels(G1, pos, edge_labels={k: f"{v:.2f}" for k,v in edge_labels.items()})
plt.show()

# ---------------------------------------
# 4. Rules for Survived = Yes (S2)
# ---------------------------------------
rules_S2 = rules_S1[rules_S1['consequents'].apply(lambda x: 'Survived_Yes' in x)]
rules_S2 = rules_S2.sort_values(by='confidence', ascending=False)
print("Rules for Survived=Yes (S2):\n", rules_S2)

# Visualization S2
G2 = nx.DiGraph()
for _, row in rules_S2.iterrows():
    for ant in row['antecedents']:
        for con in row['consequents']:
            G2.add_edge(ant, con, weight=row['confidence'])

plt.figure(figsize=(10,10))
pos = nx.spring_layout(G2, k=0.5, seed=42)
nx.draw(G2, pos, with_labels=True, node_size=2000, node_color='lightgreen', font_size=10, font_weight='bold', arrows=True)
edge_labels = nx.get_edge_attributes(G2, 'weight')
nx.draw_networkx_edge_labels(G2, pos, edge_labels={k: f"{v:.2f}" for k,v in edge_labels.items()})
plt.show()

# ---------------------------------------
# 5. Rules for Survival by Class & Age (S3)
# LHS: Sex, Class, Age; RHS: Survived=Yes
# ---------------------------------------
allowed_lhs = [
    'Sex_Male','Sex_Female',
    'Class_1st','Class_2nd','Class_3rd',
    'Age_Child','Age_Adult'
]

def lhs_valid(antecedents):
    return all(item in allowed_lhs for item in antecedents)

rules_S3 = rules_S2[rules_S2['antecedents'].apply(lhs_valid)]
rules_S3 = rules_S3.sort_values(by='confidence', ascending=False)

print("Rules S3 (Survival by Class & Age):\n", rules_S3)

# Visualization S3 - network graph
G3 = nx.DiGraph()
for _, row in rules_S3.iterrows():
    for ant in row['antecedents']:
        for con in row['consequents']:
            G3.add_edge(ant, con, weight=row['confidence'])

plt.figure(figsize=(10,10))
pos = nx.spring_layout(G3, k=0.5, seed=42)
nx.draw(G3, pos, with_labels=True, node_size=2000, node_color='orange', font_size=10, font_weight='bold', arrows=True)
edge_labels = nx.get_edge_attributes(G3, 'weight')
nx.draw_networkx_edge_labels(G3, pos, edge_labels={k: f"{v:.2f}" for k,v in edge_labels.items()})
plt.show()

# Parallel coordinates plot for S3
rules_plot = rules_S3.copy()
rules_plot['antecedents_str'] = rules_plot['antecedents'].apply(lambda x: ','.join(list(x)))
rules_plot['consequents_str'] = rules_plot['consequents'].apply(lambda x: ','.join(list(x)))

parallel_coordinates(rules_plot[['antecedents_str','confidence']], 'antecedents_str', colormap=plt.get_cmap("Set2"))
plt.xticks(rotation=90)
plt.show()

# ---------------------------------------
# Notes:
# - S1: All rules
# - S2: Rules for Survived=Yes
# - S3: Rules for Survival by Sex, Class & Age
# - Support: fraction of transactions containing both LHS and RHS
# - Confidence: likelihood that RHS occurs when LHS occurs
# - Visualization helps detect patterns
